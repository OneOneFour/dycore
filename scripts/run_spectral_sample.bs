#!/bin/bash
#
#SBATCH --job-name=dycore
#SBATCH --ntasks=8
#SBATCH --partition=twohour
#SBATCH --output=dycore_outfile.%j.out
#SBATCH --error=dycore_outfile.%j.err
#
###SBATCH --ntasks-per-node=8
#
# General Descripiton:
#  This is a samnple script to run a dycore-spectral model on Mazama HPC. The script can be fairly easily modified
#  to run in another environment, particularly an HPC environment that uses LMOD for software.
#  Note the sample scripts that came with the package (for me at least) includid compiling the code ant then running
# from a local executable... which seemed sort of silly. This version references the compiled executable via
#  environment variables. The code has been successfully compile using the Intel/19 compilers and openmpi3, mpich3, and impi.
#  gnu compilers have not (yet) been tested.
#
#
module purge
module load intel/19
COMP="intel19"
#module load openmpi_3/3.1.4
#MPI="openmpi3"
# module load mpich_3/
# MPI="mpich3"
module load impi_19/
MPI="impi19"
#
module load dycore/
#
###############################################
# User defined variables:
# user variables:
EXPERIMENT_NAME="IP_iter"     # will (or can be) part of working directory definition (see WORK_DIR below)
#
RUN_TIME_DAYS=400
DT_ATMOS=600
# some initial values for variable parameters:
PRAM_INITIAL_TEMP=274.0
PRAM_INITIAL_PERTURBATION=2.0e-7
PRAM_IP_STEP=1.0e-7         # PRAM_INITIAL_PERTURBATION=PRAM_INITIAL_PERTURBATION + SLURM_JOB_ID*PRAM_IP_STEP
PRAM_TEMP_STEP=0.0
#
# N Processors:
if [[ ! -z ${SLURM_NTASKS} ]]; then
    NPES=${SLURM_NTASKS}
else
    NPES=8
fi
if [[ -z ${NPES} ]]; then NPES=8; fi
#
# Actual computations and data will happen in WORK_DIR:
ROOT_DIR="`cd ..;pwd`"
#ROOT_DIR="`pwd`"
WORK_DIR="${ROOT_DIR}/workdir_${EXPERIMENT_NAME}"
#WORK_DIR="$SCRATCH/public/aditi/dycore/IP_iterations_Jul09"
echo "WORK_DIR: ${WORK_DIR}"
#
# Simulator setup variables; these get copied into input.nml
# (you should not need to edit these for individual perturbation runs, but you might make changes to
#  re-define your environment. These variables define some physical constants and other bits).
#
NAME_LIST=${DYCORE_DIR}/input/spectral_namelist
DIAG_TABLE=${DYCORE_DIR}/input/hs_diag_table
FIELD_TABLE=${DYCORE_DIR}/input/spectral_field_table
#
###################
# Job Array handler:
#
# math in bash is a little bit tricky.
# This is a common method but it seems to be inconsistent and picky about small < 1 numbers (likely just in the display, but still...)
# example: `echo "3.1+2.2"|bc`
# we can be more explicit by using the printf() function:
# awk "BEGIN {printf \"%.9f\n\", 2e-7 * 2.1}"
#
if [[ ! -z ${SLURM_ARRAY_TASK_ID} ]]; then
    echo "Setting up array: [${SLURM_ARRAY_TASK_ID}]"
    #PRAM_INITIAL_TEMP = `echo "${PRAM_INITIAL_TEMP} + ${SLURM_ARRAY_TASK_ID}"|bc`
    #V2=$(awk "BEGIN {printf \"%.9f\n\", 2e-7 * 2.1}")
    #PRAM_INITIAL_TEMP=$(awk "BEGIN {printf \"%.9f\n\", ${PRAM_INITIAL_TEMP} + ${PRAM_TEMP_STEP}*${SLURM_ARRAY_TASK_ID}} ")
    PRAM_INITIAL_PERTURBATION=$(awk "BEGIN {printf \"%.9E\n\", ${PRAM_INITIAL_PERTURBATION} + ${PRAM_IP_STEP}*${SLURM_ARRAY_TASK_ID} } ")
    #
    WORK_DIR=${WORK_DIR}/ary_${SLURM_ARRAY_TASK_ID}
    #
    echo "WORK_DIR(ary): ${WORK_DIR}"
fi
#######################
#
# Set up the workdir. For now, let's remove an existing workdir:
# careful!!!
if [[ -d ${WORK_DIR} ]]; then rm -rf ${WORK_DIR}; fi
#
if [[ ! -d ${WORK_DIR} ]]; then mkdir -p ${WORK_DIR}; fi
mkdir ${WORK_DIR}/INPUT
mkdir ${WORK_DIR}/RESTART
#
#--------------------------------------------------
# switch to $WORK_DIR
# construct an input.nml file
# run the MPI application
# consolidate outputs
#
# Construct the necessary inputs; put them into ${WORK_DIR}
cd ${WORK_DIR}
# create an input.nml file and ...
# set run length and time step, get input data and executable
cat > input.nml << EOF
&main_nml
     days   = ${RUN_TIME_DAYS},
     dt_atmos = ${DT_ATMOS} /

&spectral_init_cond_nml initial_temperature = ${PRAM_INITIAL_TEMP}, initial_perturbation = ${PRAM_INITIAL_PERTURBATION} /
EOF
#
# now, pull configuration data from the DyCore source into the input:
cat ${NAME_LIST} >> input.nml
cp ${DIAG_TABLE} diag_table
cp ${FIELD_TABLE} field_table
#
# input file constructed. Now run the program(s):
#
# NOTE: it might be more convenient to write and submit submit.sh script, rather than exedute inline. the
#  SLURM-foo is a bit trickier, but we could then (maybe?) handle the SLURM resrouce requests more gracefully. (??)
#
ulimit -s unlimited
#
mpirun -np ${NPES} fms.x
#
# combine netcdf files
if [[ ${NPES} -gt 1 ]]; then
  for ncfile in $( ls *.nc.0000 )
  do
    ${DYCORE_MPPNCCOMBINE}  -v -r $ncfile
  done
fi
#
