#!/bin/bash
#
#SBATCH --job-name=dycore
#SBATCH --ntasks=16
#SBATCH --partition=twohour
#SBATCH --output=dycore_outfile.%j.out
#SBATCH --error=dycore_outfile.%j.err
#SBATCH --array=0-14%3
#
#
# General Descripiton:
#  This is a samnple script to run a dycore-spectral model on Mazama HPC. The script can be fairly easily modified
#  to run in another environment, particularly an HPC environment that uses LMOD for software.
#  Note the sample scripts that came with the package (for me at least) includid compiling the code ant then running
# from a local executable... which seemed sort of silly. This version references the compiled executable via
#  environment variables. The code has been successfully compiled using the Intel/19 compilers and openmpi3, mpich3, and impi.
#  gnu compilers have not (yet) been tested.
#
echo "Begin running run_spectral_sample.bs"
#
module purge
module load intel/19
COMP="intel19"
module load openmpi_3/3.1.4
MPI="openmpi3"
# module load mpich_3/
# MPI="mpich3"
#module load impi_19/
#MPI="impi19"
#
module load dycore/
#
###############################################
# User defined variables:
# user variables:
EXPERIMENT_NAME="Phys_check"     # will (or can be) part of working directory definition (see WORK_DIR below)
RESET_WORKDIR=0
#
#RUN_TIME_DAYS=400
RUN_TIME_DAYS=800
DT_ATMOS=600
#
# use this to break up the array into sequential runs.
# NOTE this width needs to be equal to the modulus pram on the SLURM Array.
# the SLURM_ARRAY_TASK_COUNT variable???
PERTURBATION_MOD=3
#
# some initial values for variable parameters:
PRAM_INITIAL_TEMP=274.0
PRAM_INITIAL_PERTURBATION="2.0e-7"
PRAM_IP_STEP=1.0e-7         # PRAM_INITIAL_PERTURBATION=PRAM_INITIAL_PERTURBATION + SLURM_JOB_ID*PRAM_IP_STEP
PRAM_TEMP_STEP=0.0
GAUSS_HEIGHT=4000.0
#
# N Processors:
if [[ ! -z ${SLURM_NTASKS} ]]; then
    NPES=${SLURM_NTASKS}
else
    NPES=8
fi
if [[ -z ${NPES} ]]; then NPES=8; fi
#
# Actual computations and data will happen in WORK_DIR:
ROOT_DIR="`cd ..;pwd`"
#ROOT_DIR="`pwd`"
WORK_DIR="${ROOT_DIR}/workdir_${EXPERIMENT_NAME}_${GAUSS_HEIGHT}"

#WORK_DIR="$SCRATCH/public/aditi/dycore/IP_iterations_Jul09"
echo "WORK_DIR: ${WORK_DIR}"
#
# Simulator setup variables; these get copied into input.nml
# (you should not need to edit these for individual perturbation runs, but you might make changes to
#  re-define your environment. These variables define some physical constants and other bits).
#
NAME_LIST=${DYCORE_DIR}/input/spectral_namelist
DIAG_TABLE=${DYCORE_DIR}/input/hs_diag_table
FIELD_TABLE=${DYCORE_DIR}/input/spectral_field_table
#
###################
# Job Array handler:
#
# math in bash is a little bit tricky.
# This is a common method but it seems to be inconsistent and picky about small < 1 numbers (likely just in the display, but still...)
# example: `echo "3.1+2.2"|bc`
# we can be more explicit by using the printf() function:
# awk "BEGIN {printf \"%.9f\n\", 2e-7 * 2.1}"
#
if [[ ! -z ${SLURM_ARRAY_TASK_ID} ]]; then
    echo "Setting up array: [${SLURM_ARRAY_TASK_ID}] % [${SLURM_ARRAY_TASK_COUNT}]"
    #
    # perturbation index: z%n
    # sequence index: int(z/n)
    PERTURBATION_INDEX=$((${SLURM_ARRAY_TASK_ID} % ${PERTURBATION_MOD} ))
    SEQ_INDEX=$((${SLURM_ARRAY_TASK_ID} / ${PERTURBATION_MOD} ))
     #
    #PRAM_INITIAL_TEMP = `echo "${PRAM_INITIAL_TEMP} + ${SLURM_ARRAY_TASK_ID}"|bc`
    #V2=$(awk "BEGIN {printf \"%.9f\n\", 2e-7 * 2.1}")
    #PRAM_INITIAL_TEMP=$(awk "BEGIN {printf \"%.9f\n\", ${PRAM_INITIAL_TEMP} + ${PRAM_TEMP_STEP}*${SLURM_ARRAY_TASK_ID}} ")
    #PRAM_INITIAL_PERTURBATION=$(( ${PRAM_INITIAL_PERTURBATION} + ${PRAM_IP_STEP} * (${SLURM_ARRAY_TASK_ID}) ))
    PRAM_INITIAL_PERTURBATION=$(awk "BEGIN {printf \"%.9E\n\", ${PRAM_INITIAL_PERTURBATION} + ${PRAM_IP_STEP}*${PERTURBATION_INDEX} }")
    #
    WORK_DIR=${WORK_DIR}/perturb_${PERTURBATION_INDEX}
    #
    echo "WORK_DIR(ary)[${PERTURBATION_INDEX}, ${SEQ_INDEX}]: ${WORK_DIR}"
fi
#######################
#
#exit 1
# Set up the workdir. For now, let's remove an existing workdir:
# careful!!!
if [[ -d ${WORK_DIR} && ${RESET_WORKDIR} -eq 1 ]]; then rm -rf ${WORK_DIR}; fi
#
for S in ${WORK_DIR}/INPUT ${WORK_DIR}/RESTART
do
    if [[ ! -d ${S} ]]; then mkdir -p ${S}; fi
done
#if [[ ! -d ${WORK_DIR} ]]; then mkdir -p ${WORK_DIR}; fi
#mkdir ${WORK_DIR}/INPUT
#mkdir ${WORK_DIR}/RESTART
#
#--------------------------------------------------
# switch to $WORK_DIR
# construct an input.nml file
# run the MPI application
# consolidate outputs
#
# Construct the necessary inputs; put them into ${WORK_DIR}
cd ${WORK_DIR}
# create an input.nml file and ...
# set run length and time step, get input data and executable
cat > input.nml << EOF
&main_nml
     days   = ${RUN_TIME_DAYS},
     dt_atmos = ${DT_ATMOS} /

&spectral_init_cond_nml initial_temperature = ${PRAM_INITIAL_TEMP}, initial_perturbation = ${PRAM_INITIAL_PERTURBATION} /

&gaussian_topog_nml height = ${GAUSS_HEIGHT} /

EOF
#
# we can also add prams:
# namelist /gaussian_topog_nml/ height, olon, olat, wlon, wlat, rlon, rlat
#
# now, pull configuration data from the DyCore source into the input:
cat ${NAME_LIST} >> input.nml
cp ${DIAG_TABLE} diag_table
cp ${FIELD_TABLE} field_table
#
# input file constructed. Now run the program(s):
#
# NOTE: it might be more convenient to write and submit submit.sh script, rather than exedute inline. the
#  SLURM-foo is a bit trickier, but we could then (maybe?) handle the SLURM resrouce requests more gracefully. (??)
#
ulimit -s unlimited
#
# NOTE for restarts:
#  > copy RESTART/* INPUT/
# and then stash away the combined output file and/or MPI component output files. Restarting appears to overwrite, not
#  append to the component files, and mppnccombine will throw an error if the output aggregated file already exists.
#  note that the best practice is to determine if output file(s) already exist, parse them, and increment the
#  outputs, but that's more work than I have time for now, so we'll assume our job finishes.
#
mv ${WORK_DIR}/RESTART/* ${WORK_DIR}/INPUT/
mpirun -np ${NPES} fms.x
#
if [[ ! $? -eq 0 ]]; then
    echo "Error executing fms.x. Exiting."
    exit 1
fi
#
# combine netcdf files
if [[ ${NPES} -gt 1 ]]; then
  for ncfile in $( ls *.nc.0000 )
  do
    ${DYCORE_MPPNCCOMBINE}  -v -r $ncfile
    #${DYCORE_MPPNCCOMBINE}  -v $ncfile
  done
  #
  # TODO: what is the syntax to spedify the output file name or format? For now, just
  #  move them... and we know the ones we expect. really, we should be parsing
  #  ncfile...
  mv atmos_average.nc atmos_average_${SEQ_INDEX}.nc
  mv atmos_daily.nc atmos_daily_${SEQ_INDEX}.nc
fi
#


#
